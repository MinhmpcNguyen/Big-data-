import os

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    avg,
    col,
    when,
)
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.window import Window

# =====================================================
# Config
# =====================================================
APP_NAME = os.getenv("APP_NAME", "weather-silver")

S3_INPUT = os.getenv(
    "S3_INPUT",
    "s3a://hust-bucket-storage/weather_silver/",
)

SILVER_PATH = os.getenv(
    "SILVER_PATH",
    "/data/silver/weather",
)

# =====================================================
# Spark Session
# =====================================================
spark = SparkSession.builder.appName(APP_NAME).getOrCreate()
spark.sparkContext.setLogLevel("WARN")

print(f"üì• Reading from: {S3_INPUT}")
print(f"üì§ Writing silver to: {SILVER_PATH}")

# =====================================================
# 1Ô∏è‚É£ READ PARQUET (Clean Bronze-from-S3)
# =====================================================
df = spark.read.parquet(S3_INPUT)

# =====================================================
# 2Ô∏è‚É£ BASIC CLEAN + TYPE NORMALIZATION
# =====================================================
base = (
    df.select(
        col("city").cast(StringType()).alias("city"),
        col("timestamp"),
        col("description").cast(StringType()).alias("description"),
        col("temp").cast(DoubleType()).alias("temp"),
        col("humidity").cast(DoubleType()).alias("humidity"),
        col("pressure").cast(DoubleType()).alias("pressure"),
        col("wind_speed").cast(DoubleType()).alias("wind_speed"),
        col("wind_deg").cast(DoubleType()).alias("wind_deg"),
        col("cloudiness").cast(DoubleType()).alias("cloudiness"),
    )
    .filter(col("city").isNotNull())
    .filter(col("timestamp").isNotNull())
    .filter(col("temp").isNotNull())
    .dropDuplicates(["city", "timestamp"])
)

# =====================================================
# 3Ô∏è‚É£ BUSINESS LOGIC (DERIVED FEATURES)
# =====================================================

# üå°Ô∏è Temperature category (business rule)
base = base.withColumn(
    "temp_category",
    when(col("temp") < 20, "cold").when(col("temp") < 30, "warm").otherwise("hot"),
)

# =====================================================
# 4Ô∏è‚É£ WINDOW AGGREGATIONS (INTERMEDIATE SPARK SKILL)
# =====================================================

# Rolling 24h average temperature per city
w_24h = (
    Window.partitionBy("city")
    .orderBy(col("timestamp").cast("long"))
    .rowsBetween(-23, 0)
)

silver = base.withColumn(
    "avg_temp_24h",
    avg("temp").over(w_24h),
)

# =====================================================
# 5Ô∏è‚É£ PERFORMANCE OPTIMIZATION
# =====================================================

# Repartition by city ‚Üí better downstream joins
silver = silver.repartition("city")

# Cache because Silver is reused by Gold
silver.cache()

# Materialize cache (important for teaching/demo)
silver.count()

# =====================================================
# 6Ô∏è‚É£ WRITE SILVER (PARTITIONED)
# =====================================================
(silver.write.mode("overwrite").partitionBy("city").parquet(SILVER_PATH))

print("‚úÖ Silver job completed successfully")
spark.stop()
